{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6c859aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from sklearn.model_selection import train_test_split\n",
    "data=pd.read_excel(r\"C:\\Users\\Sijal\\Desktop\\New folder (2)\\t2_w_filter_ucla.xlsx\")\n",
    "data = data.dropna()\n",
    "X=data.drop(['UCLA Score (Similar to PIRADS v2)'], axis=1)\n",
    "y=data['UCLA Score (Similar to PIRADS v2)']\n",
    "# y=[]\n",
    "# for i in Y:\n",
    "#     if i<3:\n",
    "#         y.append(0)\n",
    "#     elif i==3:\n",
    "#         y.append(1)\n",
    "#     else:\n",
    "#         y.append(2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead83b61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2857f574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree with SelectKBest (ANOVA F-value): Mean accuracy=0.3762, Std=0.0125\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_classif, VarianceThreshold, RFE, SelectFromModel\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import FactorAnalysis\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "\n",
    "def interpretable_modeling(X, y):\n",
    "    # Suppress warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    # Convert X and y to pandas DataFrame and Series\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X)\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    \n",
    "    # Handle missing values in X and y\n",
    "    # Drop rows where y is NaN\n",
    "    mask = y.notnull()\n",
    "    X = X[mask].reset_index(drop=True)\n",
    "    y = y[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Identify categorical and numerical columns\n",
    "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = X.select_dtypes(include=['int64', 'float64', 'int32', 'float32']).columns.tolist()\n",
    "    \n",
    "    # Handle missing values and encode features\n",
    "    # Impute numerical columns with mean\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # Impute categorical columns with most frequent and encode\n",
    "    # Updated parameter to 'sparse_output' for scikit-learn >= 1.0\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "    ])\n",
    "\n",
    "    # Combine transformers\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numerical_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Encode target variable if it's categorical\n",
    "    if y.dtype == 'object' or y.dtype.name == 'category':\n",
    "        label_encoder = LabelEncoder()\n",
    "        y = label_encoder.fit_transform(y)\n",
    "    else:\n",
    "        y = y.values  # Ensure y is a numpy array\n",
    "\n",
    "    # Define classifiers and feature selectors\n",
    "    classifiers = {\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=123),\n",
    "        'Logistic Regression': LogisticRegression(random_state=123, max_iter=1000),\n",
    "        'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "        'Gaussian Naive Bayes': GaussianNB(),\n",
    "        'Linear Discriminant Analysis': LinearDiscriminantAnalysis()\n",
    "    }\n",
    "\n",
    "    feature_selectors = {\n",
    "        'SelectKBest (ANOVA F-value)': SelectKBest(f_classif, k='all'),\n",
    "        'Variance Threshold': VarianceThreshold(threshold=0.0),\n",
    "        'L1-based feature selection': SelectFromModel(\n",
    "            LogisticRegression(C=1.0, penalty='l1', solver='liblinear', random_state=123)\n",
    "        ),\n",
    "        'Recursive Feature Elimination': RFE(\n",
    "            estimator=LogisticRegression(random_state=123), n_features_to_select=None\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Interpretable feature reducer\n",
    "    feature_reducer = FactorAnalysis()\n",
    "\n",
    "    random_seed = 123\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_seed)\n",
    "\n",
    "    # Create a full pipeline with preprocessing and modeling\n",
    "    def full_pipeline(clf, fs):\n",
    "        return Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('feature_reducer', feature_reducer),\n",
    "            ('feature_selector', fs),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "\n",
    "    results = []\n",
    "    \n",
    "    for clf_name, clf in classifiers.items():\n",
    "        for fs_name, fs in feature_selectors.items():\n",
    "            pipeline = full_pipeline(clf, fs)\n",
    "            \n",
    "            try:\n",
    "                scores = cross_val_score(pipeline, X, y, cv=cv, error_score='raise')\n",
    "                results.append({\n",
    "                    'Classifier': clf_name,\n",
    "                    'Feature Selector': fs_name,\n",
    "                    'Accuracy': np.mean(scores),\n",
    "                    'Std': np.std(scores)\n",
    "                })\n",
    "                print(f\"{clf_name} with {fs_name}: Mean accuracy={np.mean(scores):.4f}, Std={np.std(scores):.4f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {clf_name} and {fs_name}: {e}\")\n",
    "\n",
    "    # Convert results to a DataFrame for easier viewing\n",
    "    results_df = pd.DataFrame(results)\n",
    "    print(\"\\nSummary of Results:\")\n",
    "    print(results_df)\n",
    "    return results_df\n",
    "\n",
    "# Call the function with your data\n",
    "results_df = interpretable_modeling(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e055bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
